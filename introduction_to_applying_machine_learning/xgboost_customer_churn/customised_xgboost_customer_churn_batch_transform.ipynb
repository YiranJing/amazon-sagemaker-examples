{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn with Customised XGBoost and Batch Transform\n",
    "_**Using SageMaker built-in XGBoost and Sklearn Gradient Boosted Trees to Predict Mobile Customer Departure from a Batch Transformer**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents \n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Batch Transform](#Batch-Transform)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "  1. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "    1. [Varibles Distribution](#Varibles-Distribution)\n",
    "    1. [Correlation between Features](#Correlation-between-Features)\n",
    "    1. [Variable Selection](#Variable-Selection)\n",
    "1. [Train](#Train)\n",
    "  1. [Build-in XGboost](#Build-in-XGboost)\n",
    "  1. [Customised XGboost](#Customised-XGboost)\n",
    "1. [Batch Prediction](#DBatch-Prediction)\n",
    "1. [Evaluate](#Evaluate)\n",
    "1. [Extensions](#Extensions)\n",
    "  1. [Call existing batch job in sagemaker](#Call-existing-batch-job-in-sagemaker)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "_This notebook has been adapted from an [AWS blog post](https://aws.amazon.com/blogs/ai/predicting-customer-churn-with-amazon-machine-learning/)_\n",
    "\n",
    "Losing customers is costly for any business.  Identifying unhappy customers early on gives you a chance to offer them incentives to stay.  This notebook describes using machine learning (ML) for the automated identification of unhappy customers, also known as customer churn prediction. ML models rarely give perfect predictions though, so this notebook is also about how to incorporate the relative costs of prediction mistakes when determining the financial outcome of using ML.\n",
    "\n",
    "We use an example of churn that is familiar to all of us–leaving a mobile phone operator.  Seems like I can always find fault with my provider du jour! And if my provider knows that I’m thinking of leaving, it can offer timely incentives–I can always use a phone upgrade or perhaps have a new feature activated–and I might just stick around. Incentives are often much more cost effective than losing and reacquiring a customer.\n",
    "\n",
    "---\n",
    "\n",
    "## Batch Transform\n",
    "[xgboost_customer_churn](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/xgboost_customer_churn/xgboost_customer_churn.ipynb) uses real time endpoint and does real-time prediction, batch transform uses the same mechanics as real-time hosting to generate predictions. However, unlike real-time hosted endpoints which have persistent hardware (instances stay running until you shut them down), batch transform clusters are torn down when the job completes. \n",
    "\n",
    "The main advantage of batch transformer compared to real-time hosting is that batch transformer is more convenient to handle large dataste and we donot need to worry about the running time. For example, we can call batch job in Amazon Lambda function using huge size of dataset, by contrast, we cannot handle large dataset use real-time hosting in lambda function, since lambda function runs up to 15 minutes. [Call batch job in sagemaker or Amazon Lmabda](#Call-batch-job-in-sagemaker-or-Amazon-Lmabda)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = '<your_s3_bucket_name_here>'\n",
    "prefix = 'sagemaker/DEMO-xgboost-churn-batch-transform'\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the Python libraries we'll need for the remainder of the exercise.\n",
    "\n",
    "- **Python Standard Library**: (we use them as same as what we do in jupyter notebook or python script.)\n",
    "  1. data cleaning, feature engineering \n",
    "  2. Customised XGBoost model training\n",
    "  3. model validation\n",
    "- **SageMaker Python SDK**: we used it for \n",
    "  1. Build-in XGBoost model training\n",
    "  2. Create Batch transform job\n",
    "  3. Call Batch transform job for the new data prediction (In the case that we can use the existing batch job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "import sagemaker # SageMaker Python SDK\n",
    "from sagemaker.predictor import csv_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "Mobile operators have historical records on which customers ultimately ended up churning and which continued using the service. We can use this historical information to construct an ML model of one mobile operator’s churn using a process called training. After training the model, we can pass the profile information of an arbitrary customer (the same profile information that we used to train the model) to the model, and have the model predict whether this customer is going to churn. Of course, we expect the model to make mistakes–after all, predicting the future is tricky business! But I’ll also show how to deal with prediction errors.\n",
    "\n",
    "The dataset we use is publicly available and was mentioned in the book [Discovering Knowledge in Data](https://www.amazon.com/dp/0470908742/) by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets.  Let's download and read that dataset in now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://dataminingconsultant.com/DKD2e_data_sets.zip\n",
    "!unzip -o DKD2e_data_sets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = pd.read_csv('./Data sets/churn.txt')\n",
    "pd.set_option('display.max_columns', 500) # to ensure that the dataframe show all columns \n",
    "churn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data overview\n",
    "Firstly, we need to understand the raw data size, check missing value and know the meaning of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Rows     : \" ,churn.shape[0])\n",
    "print (\"Columns  : \" ,churn.shape[1])\n",
    "print (\"\\nFeatures : \\n\" ,churn.columns.tolist())\n",
    "print (\"\\nMissing values :  \", churn.isnull().sum().values.sum())\n",
    "print (\"\\nUnique values :  \\n\",churn.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing value in the dataset. By modern standards, it’s a relatively small dataset, with only 3,333 records, where each record uses 21 attributes to describe the profile of a customer of an unknown US mobile operator. The attributes are:\n",
    "\n",
    "- `State`: the US state in which the customer resides, indicated by a two-letter abbreviation; for example, OH or NJ\n",
    "- `Account Length`: the number of days that this account has been active\n",
    "- `Area Code`: the three-digit area code of the corresponding customer’s phone number\n",
    "- `Phone`: the remaining seven-digit phone number\n",
    "- `Int’l Plan`: whether the customer has an international calling plan: yes/no\n",
    "- `VMail Plan`: whether the customer has a voice mail feature: yes/no\n",
    "- `VMail Message`: presumably the average number of voice mail messages per month\n",
    "- `Day Mins`: the total number of calling minutes used during the day\n",
    "- `Day Calls`: the total number of calls placed during the day\n",
    "- `Day Charge`: the billed cost of daytime calls\n",
    "- `Eve Mins, Eve Calls, Eve Charge`: the billed cost for calls placed during the evening\n",
    "- `Night Mins`, `Night Calls`, `Night Charge`: the billed cost for calls placed during nighttime\n",
    "- `Intl Mins`, `Intl Calls`, `Intl Charge`: the billed cost for international calls\n",
    "- `CustServ Calls`: the number of calls placed to Customer Service\n",
    "- `Churn?`: whether the customer left the service: true/false\n",
    "\n",
    "The last attribute, `Churn?`, is known as the target attribute–the attribute that we want the ML model to predict.  Because the target attribute is binary, our model will be performing binary prediction, also known as binary classification.\n",
    "\n",
    "\n",
    "Let's begin clean and exploring the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean data\n",
    "- Drop useless column: `Phone` takes on too many unique values to be of any practical use.  It's possible parsing out the prefix could have some value, but without more context on how these are allocated, we should avoid using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop('Phone', axis=1) \n",
    "churn['Area Code'] = churn['Area Code'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check duplicated rows\n",
    "print(len(churn)) # initial \n",
    "churn = churn.drop_duplicates()\n",
    "print(len(churn)) # after removing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "Produce plots and summaries to get ideas about featurer and modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target variable\n",
    "Only 14% of customers churned, so there is some class imabalance, but nothing extreme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn[\"Churn?\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorial_variable = ['State', 'Area Code', 'Int’l Plan', 'VMail Plan', ]\n",
    "\n",
    "continuous_variable = ['Account Length', 'VMail Message', 'Day Mins', 'Day Calls', 'Day Charge', \n",
    "                       'Eve Mins', 'Eve Calls', 'Eve Charge', 'Night Mins', 'Night Calls', 'Night Charge',\n",
    "                      'Intl Mins', 'Intl Calls', 'Intl Charge', 'CustServ Calls']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categoraical varibles Distribution\n",
    "The relationship between each of the features and our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours=['#1F77B4', '#FF7F0E']\n",
    "\n",
    "for column in churn.columns[:-1]: # the last column is target (churn)\n",
    "    if column in categorial_variable:\n",
    "        table=pd.crosstab(churn[\"Churn?\"],churn[column])\n",
    "        print(table)\n",
    "        print(\"--------------------------------------------------\")\n",
    "        fig, ax = plt.subplots()\n",
    "        (table.T).plot(kind='bar', alpha=0.9, color=colours, ax=ax)\n",
    "        ax.set_xlabel(column)\n",
    "        ax.set_ylabel('Churn?')\n",
    "        plt.tight_layout()\n",
    "        sns.despine()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `VMail Plan`: quite relevant to customer churn since some difference between No and Yes class. From plots we can see that, if customer have VMail Plan, then less likely to churn, compared to the customers without VMail Plan.\n",
    "- `State`: not quite relevant to customer churn since it appears to be quite evenly distributed\n",
    "- `Area code`: relevent to customer churn. From plots we can see that, if customer is in the area with code 415, then more likely to churn, compared to other area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous varibles Distribution\n",
    "The relationship between each of the features and our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in churn.select_dtypes(exclude=['object']).columns:\n",
    "    print(column)\n",
    "    hist = churn[[column, 'Churn?']].hist(by='Churn?', bins=30)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly we see that churners appear:\n",
    "- Fairly evenly distributed geographically\n",
    "- More likely to have an international plan\n",
    "- Less likely to have a voicemail plan\n",
    "- To exhibit some bimodality in daily minutes (either higher or lower than the average for non-churners)\n",
    "- To have a larger number of customer service calls (which makes sense as we'd expect customers who experience lots of problems may be more likely to churn)\n",
    "\n",
    "In addition, we see that churners take on very similar distributions for features like `Day Mins` and `Day Charge`.  That's not surprising as we'd expect minutes spent talking to correlate with charges.  Let's dig deeper into the relationships between our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between Features\n",
    "The purpose for this section is to check if continuous variables have quite simiar information with each other, i.e. Collinearity(a condition in which some of the independent variables are highly correlated) might be a problem in statistical modelling, and can also worse the performance of ML models.\n",
    "\n",
    "If some columns are quite high correlatied, we need to consider combining them or delete some columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation Matrix among continuous variables\n",
    "f,ax = plt.subplots(figsize=(15, 15))\n",
    "sns.heatmap(churn.corr(), annot=True, linewidths=.5, fmt= '.4f',ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation matrix below, we can see several features that essentially have 100% correlation with one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Selection\n",
    "From correlation plots above, we already see several features that essentially have 100% correlation with one another. Let's remove one feature from each of the highly correlated pairs: Day Charge from the pair with Day Mins, Night Charge from the pair with Night Mins, Intl Charge from the pair with Intl Mins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = churn.drop(['Day Charge', 'Eve Charge', 'Night Charge', 'Intl Charge'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature transformation for continuous variable (Optional)\n",
    "Data transformation is not important for ML models, but useful in statistical model, such as logistic model. The reasoning for data transformation is: we prefer more symmetric distribution in the statistical modelling, such as normal or uniform distribution.\n",
    "\n",
    "Let's plot the distribution of remaining continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(churn, figsize=(15, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above we can see that `intl Call`, `CustServ Calls` and  `VMail Message` show right-tail distribution, and thus we need do log or box-cox transformation to make them more symmetry distributied if using statistical modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've cleaned up our dataset, let's determine which algorithm to use. As mentioned above, there appear to be some variables where both high and low (but not intermediate) values are predictive of churn. In order to accommodate this in an algorithm like linear regression, we'd need to generate polynomial (or bucketed) terms. Instead, let's attempt to model this problem using gradient boosted trees. Amazon SageMaker provides an XGBoost container that we can use to train in a managed, distributed setting, and then host as a real-time prediction endpoint. XGBoost uses gradient boosted trees which naturally account for non-linear relationships between features and the target variable, as well as accommodating complex interactions between features.\n",
    "\n",
    "Amazon SageMaker XGBoost can train on data in either a CSV or LibSVM format. For this example, we'll stick with CSV. It should:\n",
    "\n",
    "Have the predictor variable in the first column\n",
    "Not have a header row\n",
    "But first, let's convert our categorical features into numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.get_dummies(churn)\n",
    "model_data = pd.concat([model_data['Churn?_True.'], model_data.drop(['Churn?_False.', 'Churn?_True.'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset\n",
    "And now let's split the data into training, validation, and test sets. This will help prevent us from overfitting the model, and allow us to test the models accuracy on data it hasn't already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll upload these files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "### Build-in XGboost model Training\n",
    "Moving onto training, first we'll need to specify the locations of the XGBoost algorithm containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can specify a few parameters like what type of training instances we'd like to use and how many, as well as our XGBoost hyperparameters.  A few key hyperparameters are:\n",
    "- `max_depth` controls how deep each tree within the algorithm can be built.  Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting.  There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.\n",
    "- `subsample` controls sampling of the training data.  This technique can help reduce overfitting, but setting it too low can also starve the model of data.\n",
    "- `num_round` controls the number of boosting rounds.  This is essentially the subsequent models that are trained using the residuals of previous iterations.  Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "- `eta` controls how aggressive each round of boosting is.  Larger values lead to more conservative boosting.\n",
    "- `gamma` controls how aggressively trees are grown.  Larger values lead to more conservative models.\n",
    "- `objective` logistic regression for binary classification, output probability.\n",
    "- `base_job_name` user defined model name.\n",
    "\n",
    "More detail on XGBoost's hyperparmeters can be found on their GitHub [page](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = 'DEMO-xgboost-churn' # user defined model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    base_job_name = model_name,\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customised XGboost model Training\n",
    "Use Own Algorithms (Sklearn) with Amazon SageMaker\n",
    "\n",
    "In this case, I set hyperparameters same as the build-in XGboost. But actually, if you compare the predicted result of build-in and customised models, you will find they have slightly different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Sklearn library\n",
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Split featuers and target\n",
    "train_y = train_data.iloc[:,0] \n",
    "train_X = train_data.iloc[:,1:]\n",
    "\n",
    "valid_y = validation_data.iloc[:,0]  \n",
    "valid_X = validation_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost # Sklearn library\n",
    "\n",
    "# Setup xgboost model\n",
    "customised_xgb = xgboost.XGBClassifier(max_depth=5,\n",
    "                        gamma=4,\n",
    "                        eta=0.2,        \n",
    "                        subsample=0.8,\n",
    "                        silent=0, \n",
    "                        min_child_weight=6,  \n",
    "                        num_round=100,\n",
    "                        n_estimators=200,\n",
    "                        objective='binary:logistic') # binary classification\n",
    "\n",
    "\n",
    "customised_xgb.fit(train_X, train_y, # Train it to our data\n",
    "       eval_set=[(valid_X, valid_y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the trained model file\n",
    "- The model file name must satisfy the regular expression pattern: ^\\[a-zA-Z0-9\\](-\\*[a-zA-Z0-9])*;\n",
    "- The model file also need to tar-zipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"DEMO-customised-xgboost-churn\" # user-defined model name\n",
    "customised_xgb._Booster.save_model(model_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar czvf model.tar.gz $model_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, upload the pre-trained model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fObj = open(\"model.tar.gz\", 'rb')\n",
    "key= os.path.join(prefix, model_file_name, 'model.tar.gz')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(fObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads customised Model Artifacts to SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URI where a pre-trained model is stored\n",
    "model_url = 's3://{}/{}'.format(bucket,key)\n",
    "model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customised_model_name = 'DEMO-customised-xgboost-churn' # user defined model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train customised model\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# model_uri: URI of our pre-trained model \n",
    "customised_xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    model_uri = model_url, ## load our customised model\n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    base_job_name = customised_model_name,\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "# Most hyper-parameters we have done in the pre-trained model\n",
    "customised_xgb.set_hyperparameters(num_round=50 #The number of rounds for boosting (only used in the console version of XGBoost)\n",
    "                        )\n",
    "\n",
    "# start model training\n",
    "customised_xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Prediction\n",
    "Batch Transform manages all necessary compute resources, including launching instances to deploy endpoints and deleting them afterward.\n",
    "- `batch_input` The batch input dataset used for prediction(test dataset) cannot have target column and should be saved in S3 buckets\n",
    "- `batch_output` We need to specify the path for the batch output\n",
    "\n",
    "In this example, I use buid-in model for evaluation, which is same as what we need to do for customised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_batch = test_data.iloc[:, 1:] # delete the target column\n",
    "test_data_batch.to_csv('test_data_Batch.csv',header=False, index = False)\n",
    "# upload to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'batch/test_data_Batch.csv')).upload_file('test_data_Batch.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_batch_input = 's3://{}/{}/batch/test_data_Batch.csv'.format(bucket,prefix) # test data used for prediction\n",
    "s3_batch_output = 's3://{}/{}/batch/batch-inference'.format(bucket, prefix) # specify the location of batch output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Batch job and make batch predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a transformer object from the trained model\n",
    "transformer = xgb.transformer(\n",
    "                          instance_count=1,\n",
    "                          instance_type='ml.m4.xlarge',\n",
    "                          output_path=s3_batch_output)\n",
    "\n",
    "# calls that object's transform method to create a transform job\n",
    "transformer.transform(data=s3_batch_input, data_type='S3Prefix', content_type='text/csv', split_type='Line')\n",
    "\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "we have made batch predictions and get the whole predicted putput written in `.csv.out` file. The folder of batch output is `s3_batch_output`.\n",
    "\n",
    "Now we can read in the batch output and compare with the actual data in `test data` for model evaluation. In this example, I use buid-in model for evaluation, which is same as customised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = 's3://{}/{}/batch/batch-inference/test_data_Batch.csv.out'.format(bucket,prefix)\n",
    "batch_output = pd.read_csv(batch_output, header=None, encoding = \"ISO-8859-1\") # header = none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note, due to randomized elements of the algorithm, you results may differ slightly._\n",
    "\n",
    "An important point here is that because of the `np.round()` function above we are using a simple threshold (or cutoff) of 0.5.  Our predictions from `xgboost` come out as continuous values between 0 and 1 and we force them into the binary classes that we began with.  However, because a customer that churns is expected to cost the company more than proactively trying to retain a customer who we think might churn, we should consider adjusting this cutoff.  That will almost certainly increase the number of false positives, but it can also be expected to increase the number of true positives and reduce the number of false negatives.\n",
    "\n",
    "To get a rough intuition here, let's look at the continuous values of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = np.round(batch_output) # threshold is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(y_true,y_pred):\n",
    "    f1 = metrics.f1_score(y_true, y_pred)\n",
    "    precision = metrics.precision_score(y_true, y_pred)\n",
    "    recall = metrics.recall_score(y_true, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
    "    return precision, recall, f1, accuracy, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get scores\n",
    "temp_precision, temp_recall, temp_f1, temp_accuracy, tn, fp, fn, tp = get_score(test_y, pred_y)\n",
    "output = [temp_precision,temp_recall,temp_f1,temp_accuracy,tp, fp, tn, fn]\n",
    "output = pd.Series(output, index=['precision', 'recall', 'f1', 'accuracy', 'tp', 'fp', 'tn', 'fn']) \n",
    "print(output[['accuracy', 'tp', 'fp', 'tn', 'fn']])\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 48 churners, we've correctly predicted 39 of them (true positives). And, we incorrectly predicted 4 customers would churn who then ended up not doing so (false positives).  There are also 9 customers who ended up churning, that we predicted would not (false negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuous valued predictions coming from our model tend to skew toward 0 or 1, but there is sufficient mass between 0.1 and 0.9 that adjusting the cutoff should indeed shift a number of customers' predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "### Call existing batch job in sagemaker\n",
    "\n",
    "When we already trained model, and want to re-use it for new data prediction, we can call exisiting model and create a new batch transform job. \n",
    "- `batch_input` any new cleand dataset we want to get prediction using existing model (in this example, I reuse testdata for demo)\n",
    "- `batch_output` define batch output \n",
    "- `ModelName` the name of existing model. You can find it in `Amazon Sagemaker dashboard` >- `Inference` >- `Models`. For example, DEMO-xgboost-churn-2019-08-08-03-30-05-889\n",
    "- `TransformJobName` user-defined name for the new batch transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "batch_input = 's3://{}/{}/batch/test_data_Batch.csv'.format(bucket,prefix) # test data used for prediction\n",
    "batch_output = 's3://{}/{}/batch/batch-inference/test_data_Batch.csv.out'.format(bucket,prefix)\n",
    "Modelname = '<your_model_name_here>' # the model name we already have\n",
    "transformJobName = 'DEMO-xgboost-churn-call-batch'+ strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_batch = client.create_transform_job(\n",
    "    TransformJobName=transformJobName,\n",
    "    ModelName=Modelname,\n",
    "    MaxConcurrentTransforms=0,\n",
    "    MaxPayloadInMB=6,\n",
    "    BatchStrategy='MultiRecord',\n",
    "    TransformInput={\n",
    "        'DataSource': {\n",
    "            'S3DataSource': {\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3Uri': batch_input \n",
    "            }\n",
    "        },\n",
    "        'ContentType': 'text/csv',\n",
    "        'CompressionType': 'None',\n",
    "        'SplitType': 'Line'\n",
    "    },\n",
    "    TransformOutput={\n",
    "        'S3OutputPath': batch_output,\n",
    "        'AssembleWith': 'Line'\n",
    "    },\n",
    "    TransformResources={\n",
    "        'InstanceType': 'ml.m4.xlarge',\n",
    "        'InstanceCount': 1\n",
    "    }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, go to `Amazon Sagemaker dashboard` >- `Inference` >- `Batch transform jobs`, you can find the new batch job is `InProgress`. \n",
    "\n",
    "After the batch job `Completed`, you can go to the batch output folder to download the new predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Clean-up\n",
    "\n",
    "If you're ready to be done with this notebook, please go to `Amazon Sagemaker dashboard` >- `Inference` >- `Models` and delete the corresponding model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
